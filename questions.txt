In this step, two additional model implementations were added to the package: a simple RandomForestClassifier and an XGBoostClassifier, each implemented as a separate class following the shared BaseModel interface. Because the training, evaluation, and prediction scripts already relied on the interface rather than a specific model, no structural pipeline changes were requiredâ€”only swapping the instantiated class. Both new models were trained and evaluated using the same preprocessing and feature-engineering stages. Performance comparison showed differences mainly in accuracy and generalization: Random Forest generally performed better than logistic regression on this dataset, while XGBoost offered additional improvements when tuned. All models could be interchanged simply by modifying the model class passed to the training script, which demonstrates that the class-based architecture successfully decouples model logic from the pipeline. Possible improvements include: centralizing shared model utilities, adding configuration files instead of hard-coded hyperparameters, improving logging for better experiment tracking, and introducing dependency injection to handle component swapping more cleanly